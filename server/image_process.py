import csv
import datetime
import os
import random
import shutil
import sys
import threading
import time
from dataclasses import dataclass
from pathlib import Path
from threading import Lock, Thread
from typing import Any, Dict, Optional, Sequence, Tuple

import cv2

from loguru import logger

from config.global_setting import global_setting
from util.time_util import time_util
from server.detect import IMAGE_EXTENSIONS, OnnxYoloDetector

report_logger = logger.bind(category="report_logger")


@dataclass(frozen=True)
class ModelConfig:
    tag: str
    model_path: Path
    class_names: Sequence[str]
    imgsz: int = 640
    conf_threshold: float = 0.31
    iou_threshold: float = 0.3



def _get_bundle_root() -> Path:
    """Return the root directory for resources in both dev and PyInstaller modes."""

    if getattr(sys, "frozen", False):  # PyInstaller runtime
        # ä¼˜å…ˆä½¿ç”¨ _MEIPASSï¼ˆè§£åŒ…åçš„ä¸´æ—¶ç›®å½•ï¼‰
        if hasattr(sys, "_MEIPASS"):
            return Path(sys._MEIPASS)
        # é€€å›åˆ°å¯æ‰§è¡Œæ–‡ä»¶æ‰€åœ¨ç›®å½•
        return Path(sys.executable).resolve().parent
    # å¼€å‘æ¨¡å¼ï¼šä½¿ç”¨æºç ç›®å½•
    return Path(__file__).resolve().parents[1]


def _resolve_model_dir() -> Path:
    bundle_root = _get_bundle_root()
    candidate = bundle_root / "models"
    if candidate.exists():
        return candidate

    # PyInstaller ä¸‹å…è®¸ç”¨æˆ·æŠŠ models æ–‡ä»¶å¤¹æ”¾åœ¨ exe åŒçº§ç›®å½•
    if getattr(sys, "frozen", False):
        exe_models = Path(sys.executable).resolve().parent / "models"
        if exe_models.exists():
            return exe_models

    logger.warning(f"æœªåœ¨ {candidate} æ‰¾åˆ° models ç›®å½•ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼Œè¯¥è·¯å¾„å¯èƒ½ä¸å­˜åœ¨")
    return candidate


_ROOT_DIR = _get_bundle_root()
_MODEL_DIR = _resolve_model_dir()

_MODEL_CONFIGS: Dict[str, ModelConfig] = {
    "FL": ModelConfig(tag="roach", model_path=_MODEL_DIR / "roach.onnx", class_names=["fly", "roach"]),
    "YL": ModelConfig(tag="fly", model_path=_MODEL_DIR / "fly.onnx", class_names=["fly", "roach"]),
}


class _DetectorRegistry:
    """Lazy loader and cache for per-type YOLO detectors."""

    def __init__(self) -> None:
        self._detectors: Dict[str, OnnxYoloDetector] = {}
        self._lock = Lock()

    def get(self, type_code: str) -> OnnxYoloDetector:
        type_key = type_code.upper()
        config = _MODEL_CONFIGS.get(type_key)
        if config is None:
            raise KeyError(f"Unsupported device type '{type_code}' for YOLO inference")

        with self._lock:
            detector = self._detectors.get(type_key)
            if detector is not None:
                return detector

            detector = OnnxYoloDetector(
                model_path=config.model_path,
                class_names=config.class_names,
                imgsz=config.imgsz,
                conf_threshold=config.conf_threshold,
                iou_threshold=config.iou_threshold,
            )
            self._detectors[type_key] = detector
            return detector


_DETECTORS = _DetectorRegistry()


def _resolve_device_type(device_code: str) -> Optional[str]:
    if not device_code:
        return None
    parts = device_code.split("_")
    if not parts:
        return None
    return parts[0].upper()


def analyze_image_with_yolo(image_full_path: Path, device_code: str) -> Tuple[int, str, Optional[Any]]:
    """Run YOLO inference for the given image and return detection info and annotated frame."""

    device_type = _resolve_device_type(device_code)
    if not device_type:
        report_logger.warning(f"æ— æ³•ä»è®¾å¤‡åè§£æç±»å‹ï¼Œè·³è¿‡: {device_code}")
        return 0, "unknown", None

    config = _MODEL_CONFIGS.get(device_type)
    if config is None:
        report_logger.warning(f"æœªé…ç½® {device_type} çš„æ¨¡å‹ï¼Œè·³è¿‡ {image_full_path}")
        return 0, "unknown", None

    try:
        detector = _DETECTORS.get(device_type)
    except Exception as exc:
        report_logger.error(f"åŠ è½½ {device_type} æ¨¡å‹å¤±è´¥: {exc}")
        return 0, config.tag, None

    try:
        image, detections = detector.predict_from_path(image_full_path)
        annotated = detector.annotate(image, detections)
        return len(detections), config.tag, annotated
    except FileNotFoundError:
        report_logger.error(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_full_path}")
    except ValueError as exc:
        report_logger.error(f"å›¾ç‰‡è¯»å–å¤±è´¥ {image_full_path}: {exc}")
    except Exception as exc:
        report_logger.error(f"YOLO æ¨ç†å¤±è´¥ {image_full_path}: {exc}")

    return 0, config.tag, None


def _store_processed_image(
    image_path: Path,
    annotated_image: Optional[Any],
    base_path: Path,
    record_suffix: str,
) -> Optional[Path]:
    """Persist annotated image (or original fallback) into the record directory."""

    type_code = image_path.stem.split("_")[0].upper()
    target_dir = base_path / f"{type_code}_{record_suffix}"
    try:
        target_dir.mkdir(parents=True, exist_ok=True)
    except Exception as exc:
        report_logger.error(f"åˆ›å»ºè®°å½•ç›®å½•å¤±è´¥ {target_dir}: {exc}")
        return None

    target_path = target_dir / image_path.name
    try:
        if annotated_image is not None:
            if not cv2.imwrite(str(target_path), annotated_image):
                raise IOError("cv2.imwrite è¿”å› False")
        else:
            shutil.copy2(str(image_path), str(target_path))
    except Exception as exc:
        report_logger.error(f"ä¿å­˜è¯†åˆ«ç»“æœå¤±è´¥ {image_path} -> {target_path}: {exc}")
        return None

    return target_path


# ========== å³æ—¶åˆ†æè¾…åŠ©å‡½æ•°ï¼ˆYOLO å®ç°ï¼‰ ==========
def immediate_process_single(filename: str, save_dir: str) -> None:
    """å¯¹å•å¼ åˆšä¿å­˜çš„æ–‡ä»¶ç«‹å³æ‰§è¡Œ YOLO åˆ†æå¹¶å†™å…¥/æ›´æ–° CSVã€‚"""

    try:
        base = os.path.basename(filename)
        name_parts = base.split('_')
        if len(name_parts) < 4:
            report_logger.warning(f"æ–‡ä»¶åä¸ç¬¦åˆçº¦å®šï¼Œè·³è¿‡å³æ—¶ç»Ÿè®¡: {base}")
            return

        device_code = f"{name_parts[0]}_{name_parts[1]}"
        date_fmt = name_parts[2].replace('-', '')
        time_fmt = name_parts[3].split('.')[0].replace('-', ':')
        full_path = Path(save_dir) / base

        count, tag, annotated = analyze_image_with_yolo(full_path, device_code)
        writer = global_setting.get_setting("global_report_writer")
        lock = global_setting.get_setting("report_lock")
        if writer is None or lock is None:
            report_logger.error("å…¨å±€ report_writer æœªåˆå§‹åŒ–ï¼Œæ— æ³•å³æ—¶å†™å…¥")
            return

        latest = writer.get_latest_file(writer.file_direct_path)
        if latest is None or (not os.path.exists(latest)):
            writer.file_path = (
                writer.file_direct_path
                + writer.file_name_preffix
                + time_util.get_format_file_from_time(time.time())
                + writer.file_name_suffix
            )
            writer.csv_create()
        else:
            writer.file_path = latest

        with lock:
            writer.update_data(date_fmt, time_fmt, device_code, count)
        writer.csv_close()
        server_cfg = global_setting.get_setting("server_config")
        if server_cfg:
            base_dir = Path(server_cfg['Storage']['fold_path']).resolve()
            record_suffix = server_cfg['Image_Process']['fold_suffix']
        else:
            base_dir = Path(save_dir).resolve().parent
            record_suffix = "Record"
        stored_path = _store_processed_image(full_path, annotated, base_dir, record_suffix)
        if stored_path is not None:
            try:
                full_path.unlink(missing_ok=True)
            except Exception as cleanup_exc:
                report_logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {full_path}: {cleanup_exc}")
        report_logger.info(f"å³æ—¶ç»Ÿè®¡å®Œæˆ {device_code} -> {count} ({tag})")
        done_event = global_setting.get_setting("processing_done")
        if done_event is not None:
            done_event.set()
    except Exception as exc:
        report_logger.error(f"å³æ—¶å¤„ç†å¤±è´¥ {filename}: {exc}")


class report_writing:
    """
    å°†å¤„ç†çš„åæ ‡å†™å…¥csvæ–‡ä»¶
    """

    def __init__(self, file_path, file_name_preffix, file_name_suffix):
        self.csv_file = None
        self.csv_writer = None
        self.encoding = 'gbk'
        self.max_retry_attempts = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.retry_delay = 1  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        self.file_lock = threading.Lock()  # æ–‡ä»¶æ“ä½œé”

        self.file_name_preffix = file_name_preffix
        self.file_name_suffix = file_name_suffix
        self.file_direct_path = file_path
        self.file_path = file_path + self.file_name_preffix + time_util.get_format_file_from_time(
            time.time()) + self.file_name_suffix

    def _safe_file_operation(self, operation_func, *args, **kwargs):
        """
        å®‰å…¨çš„æ–‡ä»¶æ“ä½œï¼Œå¸¦é‡è¯•æœºåˆ¶
        """
        for attempt in range(self.max_retry_attempts):
            try:
                with self.file_lock:
                    return operation_func(*args, **kwargs)
            except PermissionError as e:
                if attempt < self.max_retry_attempts - 1:
                    print(f"æ–‡ä»¶è¢«å ç”¨ï¼Œå°è¯• {attempt + 1}/{self.max_retry_attempts}ï¼Œ{self.retry_delay}ç§’åé‡è¯•...")
                    time.sleep(self.retry_delay)
                else:
                    print(f"æ–‡ä»¶æ“ä½œå¤±è´¥ï¼Œå·²é‡è¯•{self.max_retry_attempts}æ¬¡: {e}")
                    raise
            except Exception as e:
                print(f"æ–‡ä»¶æ“ä½œå‡ºç°æœªçŸ¥é”™è¯¯: {e}")
                raise

    def _create_temp_file_copy(self):
        """
        åˆ›å»ºä¸´æ—¶æ–‡ä»¶å‰¯æœ¬ç”¨äºè¯»å–
        """
        temp_file = self.file_path.replace('.csv', '_temp_read.csv')
        try:
            shutil.copy2(self.file_path, temp_file)
            return temp_file
        except Exception:
            return None

    def _cleanup_temp_file(self, temp_file):
        """
        æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        """
        try:
            if temp_file and os.path.exists(temp_file):
                os.remove(temp_file)
        except Exception:
            pass

    def get_latest_file(self, folder_path):
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
        # è·å–æ–‡ä»¶å¤¹å†…æ‰€æœ‰æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if
                 os.path.isfile(os.path.join(folder_path, f))]

        if not files:  # å¦‚æœæ–‡ä»¶å¤¹ä¸ºç©º
            return None

        # ä½¿ç”¨ max å‡½æ•°æ‰¾åˆ°ä¿®æ”¹æ—¶é—´æœ€æ–°çš„æ–‡ä»¶
        latest_file = max(files, key=os.path.getmtime)
        return latest_file

    def csv_create(self):
        def _create_operation():
            if not os.path.exists(self.file_direct_path):
                os.makedirs(self.file_direct_path)
            with open(self.file_path, mode='w', newline='', encoding=self.encoding) as file:
                writer = csv.writer(file)
                writer.writerow(["æ—¥æœŸ", "æ—¶é—´", "è®¾å¤‡å·", "æ•°é‡"])

        return self._safe_file_operation(_create_operation)

    def update_data(self, date, time, equipment_number, nums):
        # è¯»å–ç°æœ‰æ•°æ®
        current_data = self.csv_read()

        # å¦‚æœè®¾å¤‡å·å·²å­˜åœ¨ï¼Œæ›´æ–°æ•°æ®ï¼Œå¦åˆ™æ·»åŠ 
        current_data[equipment_number] = {
            "æ—¥æœŸ": date,
            "æ—¶é—´": time,
            "è®¾å¤‡å·": equipment_number,
            "æ•°é‡": nums,
        }

        # å†™å› CSV
        self.csv_write_multiple(current_data)

    def csv_read(self):
        """
        å®‰å…¨è¯»å–CSVæ–‡ä»¶ï¼Œæ”¯æŒæ–‡ä»¶è¢«å ç”¨æ—¶çš„å¤„ç†
        """

        def _read_operation():
            data = {}
            # é¦–å…ˆå°è¯•ç›´æ¥è¯»å–
            try:
                with open(self.file_path, mode='r', encoding=self.encoding) as file:
                    reader = csv.DictReader(file)
                    for row in reader:
                        if "è®¾å¤‡å·" in row.keys():
                            data[row['è®¾å¤‡å·']] = row
                return data
            except PermissionError:
                # å¦‚æœæ–‡ä»¶è¢«å ç”¨ï¼Œå°è¯•è¯»å–å‰¯æœ¬
                temp_file = self._create_temp_file_copy()
                if temp_file:
                    try:
                        with open(temp_file, mode='r', encoding=self.encoding) as file:
                            reader = csv.DictReader(file)
                            for row in reader:
                                if "è®¾å¤‡å·" in row.keys():
                                    data[row['è®¾å¤‡å·']] = row
                        return data
                    finally:
                        self._cleanup_temp_file(temp_file)
                else:
                    raise
            except FileNotFoundError:
                return data

        return self._safe_file_operation(_read_operation)

    def csv_read_not_dict(self):
        """
        å®‰å…¨è¯»å–CSVæ–‡ä»¶ä¸ºåˆ—è¡¨æ ¼å¼
        """

        def _read_operation():
            data = []
            # é¦–å…ˆå°è¯•ç›´æ¥è¯»å–
            try:
                with open(self.file_path, mode='r', encoding=self.encoding) as file:
                    reader = csv.DictReader(file)
                    for row in reader:
                        data.append(row)
                return data
            except PermissionError:
                # å¦‚æœæ–‡ä»¶è¢«å ç”¨ï¼Œå°è¯•è¯»å–å‰¯æœ¬
                temp_file = self._create_temp_file_copy()
                if temp_file:
                    try:
                        with open(temp_file, mode='r', encoding=self.encoding) as file:
                            reader = csv.DictReader(file)
                            for row in reader:
                                data.append(row)
                        return data
                    finally:
                        self._cleanup_temp_file(temp_file)
                else:
                    raise
            except FileNotFoundError:
                return data

        return self._safe_file_operation(_read_operation)

    def _generate_new_filepath(self):
        """
        ç”Ÿæˆä¸€ä¸ªå…¨æ–°çš„æ–‡ä»¶è·¯å¾„ï¼Œé¿å¼€è¢«å ç”¨çš„æ–‡ä»¶
        """
        timestamp = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
        random_suffix = random.randint(100, 999)
        new_filename = f"{self.file_name_preffix}{timestamp}_{random_suffix}{self.file_name_suffix}"
        return os.path.join(self.file_direct_path, new_filename)

    def csv_write_multiple(self, data):
        """
        å¦‚æœåŸæ–‡ä»¶è¢«å ç”¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶å†™å…¥
        """

        def _write_operation():
            target_file = self.file_path
            max_attempts = 3

            for attempt in range(max_attempts):
                try:
                    with open(target_file, mode='w', encoding=self.encoding, newline='') as file:
                        fieldnames = ['æ—¥æœŸ', 'æ—¶é—´', 'è®¾å¤‡å·', 'æ•°é‡']
                        writer = csv.DictWriter(file, fieldnames=fieldnames)
                        writer.writeheader()
                        writer.writerows(data.values())

                    # å†™å…¥æˆåŠŸï¼Œæ›´æ–°æ–‡ä»¶è·¯å¾„
                    if target_file != self.file_path:
                        old_file = os.path.basename(self.file_path)
                        new_file = os.path.basename(target_file)
                        print(f"âœ… åŸæ–‡ä»¶ {old_file} è¢«å ç”¨ï¼Œæ•°æ®å·²å†™å…¥æ–°æ–‡ä»¶ {new_file}")
                        self.file_path = target_file
                    return

                except PermissionError:
                    if attempt < max_attempts - 1:
                        # ç”Ÿæˆæ–°çš„æ–‡ä»¶è·¯å¾„
                        target_file = self._generate_new_filepath()
                        print(f"ğŸ”„ æ–‡ä»¶è¢«å ç”¨ï¼Œå°è¯•å†™å…¥æ–°æ–‡ä»¶: {os.path.basename(target_file)}")
                    else:
                        raise Exception(f"æ— æ³•å†™å…¥æ–‡ä»¶ï¼Œå·²å°è¯• {max_attempts} æ¬¡")

        return self._safe_file_operation(_write_operation)

    def csv_write(self, date, time, equipment_number, nums):
        def _write_operation():
            with open(self.file_path, mode='a', newline='', encoding=self.encoding) as file:
                writer = csv.writer(file)
                writer.writerow([date, time, equipment_number, nums])

        return self._safe_file_operation(_write_operation)

    def csv_close(self):
        if self.csv_file is not None:
            self.csv_file.close()
            self.csv_file = None
            self.csv_writer = None

    def set_retry_config(self, max_attempts=5, delay=1):
        """
        é…ç½®é‡è¯•å‚æ•°
        """
        self.max_retry_attempts = max_attempts
        self.retry_delay = delay
class Img_process(Thread):
    """å›¾åƒè¯†åˆ«ç®—æ³•çº¿ç¨‹ (ä½¿ç”¨ ONNX YOLO æ¨ç†)."""

    def __init__(
        self,
        types,
        temp_folder,
        record_folder,
        report_fold_name,
        report_file_name_preffix,
        report_file_name_suffix,
    ):
        super().__init__()

        server_cfg = global_setting.get_setting('server_config')
        base_path_raw = server_cfg['Storage']['fold_path'] if server_cfg else './data_smart_device/'
        self.base_path = Path(base_path_raw).resolve()
        self.types = [t.upper() for t in types]
        self.temp_folder = temp_folder.strip('/\\')
        self.record_folder = record_folder.strip('/\\')

        for t in self.types:
            (self.base_path / f"{t}_{self.temp_folder}").mkdir(parents=True, exist_ok=True)
            (self.base_path / f"{t}_{self.record_folder}").mkdir(parents=True, exist_ok=True)

        self.report_folder_name = report_fold_name.strip('/\\')
        report_dir = self.base_path / self.report_folder_name
        report_dir.mkdir(parents=True, exist_ok=True)

        self.report_file_name_preffix = report_file_name_preffix
        self.report_file_name_suffix = report_file_name_suffix
        report_dir_with_sep = str(report_dir) + os.sep
        self.data_save = report_writing(
            file_path=report_dir_with_sep,
            file_name_preffix=report_file_name_preffix,
            file_name_suffix=report_file_name_suffix,
        )
        self.running = False

    def get_image_files(self) -> Sequence[Path]:
        """è·å–ä¸´æ—¶ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶ï¼ˆéé€’å½’ï¼‰ã€‚"""

        image_files: list[Path] = []
        for t in self.types:
            temp_dir = self.base_path / f"{t}_{self.temp_folder}"
            temp_dir.mkdir(parents=True, exist_ok=True)
            for entry in temp_dir.iterdir():
                if entry.is_file() and entry.suffix.lower() in IMAGE_EXTENSIONS:
                    image_files.append(entry)
        image_files.sort()
        return image_files

    def image_process_remains(self) -> None:
        if self.has_files():
            logger.info("å¤„ç†ä¸Šæ¬¡ temp æ–‡ä»¶å¤¹æœªå¤„ç†å®Œçš„æ•°æ®")
            self.image_processing()

    def has_files(self) -> bool:
        return any(self.get_image_files())

    # è¿è¡Œç»“æŸ
    def join(self, timeout: Optional[float] = None):
        self.running = False
        try:
            super().join(timeout)
        except RuntimeError:
            pass

    def stop(self):
        self.running = False
        condition = global_setting.get_setting("condition")
        if condition is not None:
            with condition:
                condition.notify_all()

    # å¯åŠ¨ï¼Œè·å–ä¸€å¸§

    def run(self):
        self.running = True
        condition = global_setting.get_setting("condition")
        if condition is None:
            logger.error("å›¾åƒå¤„ç†çº¿ç¨‹ç¼ºå°‘åŒæ­¥æ¡ä»¶å˜é‡ï¼Œçº¿ç¨‹é€€å‡º")
            return

        server_cfg = global_setting.get_setting("server_config")
        try:
            delay = float(server_cfg['Image_Process']['delay']) if server_cfg else 0.0
        except Exception:
            delay = 0.0

        poll_interval = max(0.0, delay)
        while self.running:
            if not self.has_files():
                with condition:
                    condition.wait(timeout=poll_interval or None)
                if not self.running:
                    break
                # é†’æ¥åå†æ¬¡æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ï¼Œæ²¡æœ‰åˆ™ç»§ç»­ç­‰å¾…
                if not self.has_files():
                    continue

            self.image_processing()

            if poll_interval > 0:
                time.sleep(poll_interval)

    def image_processing(self) -> None:
        images = list(self.get_image_files())
        if not images:
            report_logger.warning("æš‚æœªæ£€æµ‹åˆ°å¾…å¤„ç†çš„ FL/YL å›¾åƒ")
            return

        latest_file = self.data_save.get_latest_file(self.data_save.file_direct_path)
        if latest_file is None:
            self.data_save.csv_create()
        else:
            self.data_save.file_path = latest_file

        processed_any = False
        event = global_setting.get_setting("processing_done")

        for image_path in images:
            metadata = self._parse_image_metadata(image_path)
            if metadata is None:
                report_logger.warning(f"æ–‡ä»¶åä¸ç¬¦åˆçº¦å®šï¼Œè·³è¿‡: {image_path.name}")
                image_path.unlink(missing_ok=True)
                continue

            device_code, date_fmt, time_fmt = metadata
            count, tag, annotated = self.image_handle(image_path, device_code)
            self.data_save.update_data(date_fmt, time_fmt, device_code, count)
            report_logger.info(f"å®Œæˆ {device_code} æ•°æ®åˆ†æ -> {count} ({tag})")
            self._archive_file(image_path, annotated)
            if event is not None:
                try:
                    event.set()
                except Exception:
                    logger.debug("processing_done per-image set failed", exc_info=True)
            processed_any = True

        self.data_save.csv_close()

        if processed_any:
            cycle_received = global_setting.get_setting("cycle_received_uids")
            if cycle_received is not None:
                cycle_received.clear()
            global_setting.set_setting("data_buffer", [])
            global_setting.set_setting("cycle_start_time_image", time.time())

    def _parse_image_metadata(self, image_path: Path) -> Optional[Tuple[str, str, str]]:
        parts = image_path.stem.split('_')
        if len(parts) < 4:
            return None
        device_code = f"{parts[0]}_{parts[1]}"
        date_fmt = parts[2].replace('-', '')
        time_fmt = parts[3].replace('-', ':')
        return device_code, date_fmt, time_fmt

    def _archive_file(self, image_path: Path, annotated_image: Optional[Any]) -> None:
        stored_path = _store_processed_image(image_path, annotated_image, self.base_path, self.record_folder)
        if stored_path is not None:
            try:
                image_path.unlink(missing_ok=True)
            except Exception as exc:
                report_logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {image_path}: {exc}")
            return

        type_code = image_path.stem.split('_')[0].upper()
        target_dir = self.base_path / f"{type_code}_{self.record_folder}"
        try:
            target_dir.mkdir(parents=True, exist_ok=True)
            shutil.move(str(image_path), str(target_dir / image_path.name))
        except Exception as exc:
            report_logger.error(f"å½’æ¡£ {image_path} å¤±è´¥: {exc}")

    def image_handle(self, image_path: Path, device_code: str) -> Tuple[int, str, Optional[Any]]:
        logger.info(f"å¤„ç†æ•°æ® {image_path}")
        return analyze_image_with_yolo(image_path, device_code)